{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework1\n",
    "\n",
    "Please upload to Moodle a .zip archive containing your Jupyter Notebook with solutions and all data required to reproduce your solutions. \n",
    "\n",
    "Please also prepare a requirements.txt file which lists all the packages that you have used for your homework, one package per line. This will allow us to install all required packages in one go, by using \"pip install -r requirements.txt\".\n",
    "\n",
    "Please name your .zip archive using your full name and student id as follows - *Firstname_Lastname_12345678_COMP47350_Homework1.zip*. \n",
    "\n",
    "For your Notebook, please split the code and explanations into many little cells so it is easy to see and read the results of each step of your solution. Please remember to name your variables and methods with self-explanatory names. Please remember to write comments and where needed, justifications, for the decisions you make and code you write. Feel free to revisit *tips_to_keep_your_ipython_notebook_readable_and_easy_to_debug.html* provided on Moodle.\n",
    "\n",
    "Your code and analysis is like a story that awaits to be read, make it a nice story please.\n",
    "\n",
    "The accepted file formats for the homework are:\n",
    "    - .ipynb\n",
    "    - .zip\n",
    "    - .pdf\n",
    "    - .csv\n",
    "    \n",
    "Please keep the whole code in a single notebook. Usage of external tools/files is discouraged for portability reasons. Files in any other format but mentioned above can be used but will be ignored and not considered for the submission (including .doc, .rar, .7z, .pages, .xlsx, .tex etc.). \n",
    "Any image format is allowed to be used as far as the images appear embedded in your report (.ipynb or .pdf or .html).\n",
    "\n",
    "**Deadline: Friday, 8 March, 2019, midnight.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "This homework focuses on data understanding and preparation for a particular problem and dataset.\n",
    "The problem and data come from a credit scoring company concerned with reducing credit repayment risk. The company wants to use the data collected about their customers to build a data analytics solution for credit risk prediction.\n",
    "The fundamental task is to use the information about the applicant in their credit report to predict whether they will repay their credit within 2 years. The target variable to predict is a binary variable called RiskPerformance. The value “Bad” indicates that a consumer was 90 days past due or worse at least once over a period of 24 months from when the credit account was opened. The value “Good” indicates that they have made their payments without ever being more than 90 days overdue. The dataset we work with is a sample of the data used in this data challenge: https://community.fico.com/s/explainable-machine-learning-challenge?tabset-3158a=2\n",
    "\n",
    "Each student will work with a different subset of the data. The CSV file is named using the format: **CreditRisk-[your-student-number].csv**, e.g., **CreditRisk-12345678.csv** is the data file for a student with number 12345678. You need to work with the CSV file corresponding to your student number. There are 4 parts for this homework. Each part has an indicative maximum percentage given in brackets, e.g., part (1) has a maximum of 40% shown as [40].\n",
    "\n",
    "\n",
    "\n",
    "(1). [40] Prepare a data quality report for your CSV file. Below you have a set of guideline steps to help you in this process.\n",
    "    - Check how many rows and columns your CSV has.\n",
    "    - Print the first and the last 5 rows.\n",
    "    - Convert the features to their appropriate data types (e.g., decide which features are more appropriate as \n",
    "    continuos and which ones as categorical types). \n",
    "    - Drop duplicate rows and columns, if any.\n",
    "    - Drop constant columns, if any.\n",
    "    - Save your updated/cleaned data frame to a new csv file.\n",
    "  \n",
    "    For the updated CSV and data frame (after column/row removal):\n",
    "    - Prepare a table with descriptive statistics for all the continuous features.\n",
    "    - Prepare a table with descriptive statistics for all the categorical features.\n",
    "    - Plot histograms for all the continuous features.\n",
    "    - Plot box plots for all the continuous features.\n",
    "    - Plot bar plots for all the categorical features.\n",
    "    - Discuss your initial findings.\n",
    "    - Save the initial discussion of your findings into a single data quality report PDF file.                    \n",
    "    The PDF report should focus on the key issues identified in the data and discuss potential strategies              to handle them. Simple listing of tables and plots without discussion and justification will not receive full marks. \n",
    "\n",
    "(2). [30] Prepare a data quality plan for the cleaned CSV file. \n",
    "    - Mark down all the features where there are potential problems or data quality issues.\n",
    "    - Propose solutions to deal with the problems identified. Explain why did you choose one solution over \n",
    "    potentially many other.\n",
    "    - Apply your solutions to obtain a new CSV file where the identified data quality issues were addressed. \n",
    "    - Save the new CSV file with a self explanatory name. \n",
    "    - Save the data quality plan to a single PDF file.\n",
    "        \n",
    "(3). [15] Exploring relationships between feature pairs:\n",
    "    - Choose a subset of features you find promising and plot pairwise feature interactions (e.g., \n",
    "    continuous-continuous feature plot or continuous-categorical plots or correlation plots). \n",
    "    Explain your choices.\n",
    "    - Discuss your findings from the plots above. Do you find any features or feature combinations that are \n",
    "    indicative of the target outcome? Explain in plain words (a short paragraph) the story of your\n",
    "    findings so far.\n",
    "    \n",
    "(4). [15] Transform, extend or combine the existing features to create a few new features (at least 3) with the aim to better capture the problem domain and the target outcome. Justify the steps and choices you are making. Add these features to your clean dataset and save it as a CSV file with a self explanatory name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set info\n",
    "\n",
    "Special values are assigned to variables that have different reasons why a value is not available. \n",
    "\n",
    "#### -9 No Bureau Record or No Investigation\n",
    "\n",
    "No record means no credit history/score information is available.\n",
    "\n",
    "#### -8 No Usable/Valid Accounts Trades or Inquiries\n",
    "\n",
    "Usable or valid for Accounts/Trades means inactive, or very old.\n",
    "For inquiries, this can mean that the account has no “hard” inquiries, i.e. you are not actively searching for credit. However, if your bank pulled your credit score to send you a pre-approved credit card, the bank’s inquiry is deemed not valid.\n",
    "\n",
    "#### -7 Condition not Met (e.g. No Inquiries, No Delinquencies) \n",
    "\n",
    "“Condition not met,” which implies that the feature/variable searched for a certain event’s occurrence in the data, and that event was not found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required packages\n",
    "#Import package pandas for data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import package numpy for numeric computing\n",
    "import numpy as np\n",
    "\n",
    "# Import package matplotlib for visualisation/plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from a csv file\n",
    "df = pd.read_csv('CreditRisk-18205940.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many rows and columns this dataframe has\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing data all features\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 5 rows...\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...and last 5 rows\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the features to their appropriate data types\n",
    "We first explore the data, then decide which columns should be treated as 'continuous' and which should be treated as 'categorical'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first categorical feature is RiskPerformance so we treat it as such\n",
    "df['RiskPerformance'] = df['RiskPerformance'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary shows that the columns <i>**MaxDelq2PublicRecLast12M**</i> & <i>**MaxDelqEver**</i> refer to specific categories so we should consider them accordingly.\n",
    "\n",
    "As a first step we replace each value with its corresponding category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_MaxDelq2PublicRecLast12M ={0:\"derogatory comment\",1:\"120+ days delinquent\", 2:\"90 days delinquent\", 3:\"60 days delinquent\", 4:\"30 days delinquent\", 5:\"unknown delinquency\", 6:\"unknown delinquency\", 7:\"current and never delinquent\", 8:\"all other\", 9:\"all other\"}\n",
    "dict_MaxDelq2PublicRecLast12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary also for attribute MaxDelqEver\n",
    "dict_MaxDelqEver ={1:\"No such value\", 2:\"derogatory comment\", 3:\"120+ days delinquent\", 4:\"90 days delinquent\", 5:\"60 days delinquent\", 6:\"30 days delinquent\", 7:\"unknown delinquency\", 8:\"current and never delinquent\", 9:\"all other\"}\n",
    "dict_MaxDelqEver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the loc method the first term indicates what I am searching i.e values of MaxDelq2PublicRecLast12M = 0\n",
    "# the second term indicates the column where I want to perform an action i.e. MaxDelq2PublicRecLast12M\n",
    "# = stands for the action I want to perform\n",
    "for key, value in dict_MaxDelq2PublicRecLast12M.items():\n",
    "    df.loc[df['MaxDelq2PublicRecLast12M'] == key, 'MaxDelq2PublicRecLast12M'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use same technique for attribute MaxDelqEver\n",
    "\n",
    "for key, value in dict_MaxDelqEver.items():\n",
    "    df.loc[df['MaxDelqEver'] == key, 'MaxDelqEver'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the convert into categorical the two modified features\n",
    "df['MaxDelq2PublicRecLast12M'] = df['MaxDelq2PublicRecLast12M'].astype('category')\n",
    "df['MaxDelqEver'] = df['MaxDelqEver'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also decide to treat as categorical the features that contains the special values -7 (<i>**MSinceMostRecentDelq**</i> & <i>**MSinceMostRecentInqexcl7days**</i>). \n",
    "\n",
    "This becuase the special value means 'Condition not Met' (e.g. No Inquiries, No Delinquencies) and for these 2 features it may indicate a link to the target outcome, i.e. if the instance is set to 'Bad' or 'Good'.\n",
    "\n",
    "For the feature <i>**MSinceMostRecentDelq**</i> we therefore assign a categorical value of 'No delinquencies' to replace the special values -7 and we split the continuos values into bins that represent a 10 months interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([-7], 'No delinquencies')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(0,10)], '0-9 months')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(10,20)], '10-19 months')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(20,30)], '20-29 months')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(30,40)], '30-39 months')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(40,50)], '40-49 months')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(50,60)], '50-59 months')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(60,70)], '60-69 months')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(70,80)], '70-79 months')\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace([range(80,100)], '80+ months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MSinceMostRecentDelq'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature <i>**MSinceMostRecentInqexcl7days**</i> we assign a categorical value of 'No inquiries' to replace the special values -7 and we split the continuos values into bins that represent a 5 months interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace([-7], 'No inquiries')\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace([range(0,5)], '0-4 months')\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace([range(5,10)], '5-9 months')\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace([range(10,15)], '10-14 months')\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace([range(15,20)], '15-19 months')\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace([range(20,40)], '20+ months')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MSinceMostRecentInqexcl7days'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the type of the modified feature\n",
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].astype('category')\n",
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check and remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column to find duplicate rows\n",
    "df[\"is_duplicate\"]= df.duplicated()\n",
    "\n",
    "# show first diplicate rows\n",
    "df[df['is_duplicate'] == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duplicate rows have values -9 (No Bureau Record or No Investigation) for each attribute we can therefore drop all the rows as they don't contain any relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['is_duplicate'] == True].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if rows were dropped\n",
    "df[df['is_duplicate'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dropped the duplicate rows, now we also drop the remaining rows with all values -9\n",
    "df.loc[(df['ExternalRiskEstimate'] == -9) & (df['MSinceOldestTradeOpen'] == -9) & (df['MSinceOldestTradeOpen'] == -9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[(df.ExternalRiskEstimate == -9) & (df.MSinceOldestTradeOpen == -9) & (df.MSinceOldestTradeOpen == -9)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now drop also the new created column \"is_duplicate\" since we don't need it anymore\n",
    "df = df.drop('is_duplicate',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look for possible columns to drop by extracting the number of unique values for each attribute (i.e. cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature, UniqueValues\") \n",
    "for column in df.columns:\n",
    "    print(column + \", \" + str(len(df[column].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no attributes with cardinality 1 and there are no duplicate columns so at this stage no column should be dropped.\n",
    "\n",
    "We can now save the first version of the update/cleaned dataframe on a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updates/cleaned data frame to a new csv file.\n",
    "#df.to_csv(\"CreditRisk_clean_round1_25Feb2019_DataQualityReport.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables with descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the update version of the dataset\n",
    "df = pd.read_csv('CreditRisk_clean_round1_25Feb2019_DataQualityReport.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats for continuous features only.\n",
    "data_quality_report_numeric_table = df.select_dtypes(['int64']).describe().T\n",
    "\n",
    "#rounding values \n",
    "data_quality_report_numeric_table = data_quality_report_numeric_table.round(2)\n",
    "\n",
    "# Print data quality report for numeric features to a file\n",
    "#data_quality_report_numeric_table.to_csv(\"CreditRisk-DataQualityReport-NumericFeatures-Table.csv\", index_label='Feature')\n",
    "\n",
    "#show table\n",
    "data_quality_report_numeric_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats for categorical features only.\n",
    "data_quality_report_category_table = df.select_dtypes(['object']).describe().T\n",
    "\n",
    "# Print data quality report for categorical features to a file\n",
    "#data_quality_report_category_table.to_csv(\"CreditRisk-DataQualityReport-CategoryFeatures-Table.csv\", index_label='Feature')\n",
    "\n",
    "#show table\n",
    "data_quality_report_category_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graphs\n",
    "#### Plot histograms for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the histogram of all numeric features at the same time.\n",
    "plt.figure()\n",
    "df.hist(figsize=(20, 20), bins=12)\n",
    "# We can save the plot into a file\n",
    "#plt.savefig('CreditRisk_DataQualityReport_NumericFeatures_Histograms.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot box plots for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraxt only continuous features\n",
    "numeric_columns = df.select_dtypes(['int64'])\n",
    "# We can plot the box plots of all numeric features at the same time using a loop\n",
    "for elem in numeric_columns:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    df[elem].plot(kind='box')\n",
    "    #plt.savefig(elem+'_DataQualityReport_NumericFeatures_Boxplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot bar plots for all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraxt only continuous features\n",
    "category_columns = df.select_dtypes(['object'])\n",
    "# Plot a barplot for categorical features\n",
    "for elem in category_columns:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    df[elem].value_counts().plot(kind='bar')\n",
    "    #plt.savefig(elem+'_DataQualityReport_CategoryFeatures_Bar.png')\n",
    "    plt.suptitle(elem, x=0.5, y=0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Data Quality Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from a csv file\n",
    "df = pd.read_csv('CreditRisk_clean_round1_25Feb2019_DataQualityReport.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code allows us to know the percentage of each value for the desired feature\n",
    "# we can easily find the percentage of missing values (or any other)\n",
    "\n",
    "#df['ExternalRiskEstimate'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Categorical Feature     | Data Quality Issue   | Handling Strategy         |\n",
    "|-------------------------|----------------------|------------------------   |\n",
    "| RiskPerformance          | None | Keep as is              |\n",
    "| MSinceMostRecentDelq       |  Special Value -8 (2%)    |   Imputation       |\n",
    "|MSinceMostRecentInqexcl7days|  Special Value -8 (4%) | Imputation|\n",
    "|MaxDelqEver           |None         | Keep as is              |\n",
    "|MaxDelq/PublicRecLast12M         |   None       | Keep as is              |\n",
    "\n",
    "\n",
    "| Numeric Feature         | Data Quality Issue   | Handling Strategy         |\n",
    "|-------------------------|----------------------|------------------------   |\n",
    "|ExternalRiskEstimate     |   Special Value -9 (0.2%)    |      Imputation (mean)     |\n",
    "|MSinceOldestTradeOpen   | Special Value -8 (3%) & Outliers (High) | Imputation (max) & Clamp   |\n",
    "|MSinceMostRecentTradeOpen    |   Outliers (High)     | Clamp transformation         |\n",
    "|AverageMInFile            |Outliers (High) |  Clamp transformation          |\n",
    "|NumSatisfactoryTrades              | Outliers (High) |  Clamp transformation            |\n",
    "| NumTrades60Ever2DerogPubRec         | Outliers (High) |  Clamp transformation            |\n",
    "|NumTrades90Ever2DerogPubRec        |   Outliers (High) |  Clamp transformation            |\n",
    "|PercentTradesNeverDelq            |    Outliers (Low) |  Clamp transformation          |\n",
    "|NumTotalTrades |Outliers (High) |  Clamp transformation  |\n",
    "|NumTradesOpeninLast12M |Outliers (High) |  Clamp transformation|\n",
    "|PercentInstallTrades |Outliers (High) |  Clamp transformation|\n",
    "|NumInqLast6M|Outliers (High) |  Clamp transformation|\n",
    "|NumInqLast6Mexcl7days|Outliers (High) |  Clamp transformation|\n",
    "|NetFractionRevolvingBurden| Special Value -8 (2%) & outlier (high) |      Imputation (median)     |\n",
    "|NetFractionInstallBurden| Special Value -8 (34%) | Drop Feature |\n",
    "|NumRevolvingTradesWBalance|Special Value -8 (2%) & Outliers (High)|Imputation (mean) & Clamp   |\n",
    "|NumInstallTradesWBalance|Special Value -8 (9%) & Outliers (High)|Imputation (mean) & Clamp   |\n",
    "|NumBank2NatlTradesWHighUtilization|Special Value -8 (7%) & Outliers (High)|Imputation (mean) & Clamp   |\n",
    "|PercentTradesWBalance | Special Value -8 (0.2%) & Outliers (Low)  |      Imputation (mean)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSinceMostRecentDelq\n",
    "Months Since Most Recent Delinquency\n",
    "\n",
    "Special value -8 (2%)\n",
    "\n",
    "-8 for Accounts/Trades means inactive, or very old.\n",
    "\n",
    "Since the invalid data in quite low, we use imputation to assign this value to its explicit meaning.  \n",
    "We decide to replace the values -8 because, even if the account is very old, some delinquency could have happened in the past and dropping every row would cause an excessive data loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MSinceMostRecentDelq'] = df['MSinceMostRecentDelq'].replace(['-8'], 'No Usable/Valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSinceMostRecentInqexcl7days\n",
    "Months Since Most Recent Inq excl 7days \n",
    "\n",
    "Special value -8 (4%)\n",
    "\n",
    "-8 means no Usable/Valid Accounts Trades or Inquiries\n",
    "\n",
    "For inquiries, this can mean that the account has no “hard” inquiries, i.e. you are not actively searching for credit. However, if your bank pulled your credit score to send you a pre-approved credit card, the bank’s inquiry is deemed not valid.\n",
    "\n",
    "For this feature we use a similar approach as the one adopted for the feature <i>**MSinceMostRecentDelq**</i>.\n",
    "Since the invalid data in quite low, we use imputation to assign this value to its explicit meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MSinceMostRecentInqexcl7days'] = df['MSinceMostRecentInqexcl7days'].replace(['-8'], 'no “hard” inquiries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExternalRiskEstimate\n",
    "Consolidated version of risk markers. The feature represents a flag for the target outcome.\n",
    "\n",
    "\n",
    "Missing data (0.2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the special value to null so its negative value would not impact the mean value of the feature.\n",
    "df['ExternalRiskEstimate'] = df['ExternalRiskEstimate'].replace(-9,np.nan)\n",
    "#since missing data is very low we can use imputation and replace missing values with the mean\n",
    "df['ExternalRiskEstimate'] = df['ExternalRiskEstimate'].replace(np.nan, np.round(df['ExternalRiskEstimate'].mean(), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSinceOldestTradeOpen\n",
    "Months Since Oldest Trade Open\n",
    "\n",
    "Special Value -8 (3%) & outliers (high)\n",
    "\n",
    "-8 for Accounts/Trades means inactive, or very old.\n",
    "Since -8 stands for a very old value, we decide to replace using inputation the special value with the max value of the feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since data with this special values is very low we can use imputation and replace these values with the max\n",
    "df['MSinceOldestTradeOpen'] = df['MSinceOldestTradeOpen'].replace([-8], df['MSinceOldestTradeOpen'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to apply clamping using IQR to the desired feature\n",
    "def Clamping_IQR(feature):\n",
    "    \n",
    "    F = df[feature]\n",
    "    \n",
    "    #set parameters\n",
    "    max_F = np.max(F)\n",
    "    p_75 = np.percentile(F, 75)\n",
    "    p_25 = np.percentile(F, 25)\n",
    "    min_F = np.min(F)\n",
    "    iqr = p_75 - p_25\n",
    "    \n",
    "    #set thresholds\n",
    "    lower = np.max([min_F, p_25 - 1.5 * iqr])\n",
    "    upper = np.min([max_F, p_75 + 1.5 * iqr])\n",
    "    \n",
    "    #replace values below and above the thresholds\n",
    "    df.loc[df[feature] < lower, feature] = lower\n",
    "    df.loc[df[feature] > upper, feature] = upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping and restrict the values to a upper a lower threshold using quartiles and IQR\n",
    "Clamping_IQR('MSinceOldestTradeOpen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['MSinceOldestTradeOpen'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSinceMostRecentTradeOpen\n",
    "Months Since Most Recent Trade Open\n",
    "\n",
    "outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping\n",
    "Clamping_IQR('MSinceMostRecentTradeOpen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['MSinceMostRecentTradeOpen'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AverageMInFile\n",
    "Average Months in File\n",
    "\n",
    "outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping\n",
    "Clamping_IQR('AverageMInFile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['AverageMInFile'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumSatisfactoryTrades\n",
    "Number Satisfactory Trades\n",
    "\n",
    "outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping\n",
    "Clamping_IQR('NumSatisfactoryTrades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumSatisfactoryTrades'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumTrades60Ever/DerogPubRec\n",
    "Number Trades 60+ Ever\n",
    "\n",
    "outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping\n",
    "Clamping_IQR('NumTrades60Ever2DerogPubRec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumTrades60Ever2DerogPubRec'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumTrades90Ever/DerogPubRec\n",
    "Number Trades 90+ Ever\n",
    "\n",
    "outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping but we set upper and lower using mean and std\n",
    "\n",
    "F = df['NumTrades90Ever2DerogPubRec']\n",
    "\n",
    "max_F = np.max(F)\n",
    "p_75 = np.percentile(F, 75)\n",
    "p_25 = np.percentile(F, 25)\n",
    "min_F = np.min(F)\n",
    "iqr = p_75 - p_25\n",
    "\n",
    "lower = np.max([min_F, p_25 - 1.5 * iqr])\n",
    "\n",
    "mean = np.round(np.mean(F), 1)\n",
    "std = np.round(np.std(F), 1)\n",
    "\n",
    "upper = mean + 2 * std\n",
    "\n",
    "# replace values below and above the thresholds\n",
    "df.loc[df['NumTrades90Ever2DerogPubRec'] < lower, 'NumTrades90Ever2DerogPubRec'] = lower\n",
    "df.loc[df['NumTrades90Ever2DerogPubRec'] > upper, 'NumTrades90Ever2DerogPubRec'] = upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumTrades90Ever2DerogPubRec'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PercentTradesNeverDelq\n",
    "Percent Trades Never Delinquent\n",
    "\n",
    "outliers (low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping\n",
    "Clamping_IQR('PercentTradesNeverDelq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['PercentTradesNeverDelq'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumTotalTrades\n",
    "Number of Total Trades (total number of credit accounts)\n",
    "\n",
    "ouliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping\n",
    "Clamping_IQR('NumTotalTrades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumTotalTrades'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumTradesOpeninLast12M\n",
    "Number of Trades Open in Last 12 Months\n",
    "\n",
    "outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping\n",
    "Clamping_IQR('NumTradesOpeninLast12M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumTradesOpeninLast12M'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PercentInstallTrades\n",
    "Percent Installment Trades.\n",
    "\n",
    "Installment trade accounts involve agreements you make to pay an account over time. These accounts show your original and current balance on your credit report, as well as the amount you’re required to pay each month. Unless an installment account is new, your current balance should be less than the original balance as long as you’re making payments on time. Examples of common installment accounts include auto loans, mortgages and personal loans from banks or finance companies. (source https://budgeting.thenest.com/open-trades-credit-report-23674.html).\n",
    "\n",
    "Outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use clamping\n",
    "Clamping_IQR('PercentInstallTrades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['PercentInstallTrades'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumInqLast6M\n",
    "Number of Inquiries Last 6 Months\n",
    "\n",
    "outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use clamping to remove outliers\n",
    "Clamping_IQR('NumInqLast6M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumInqLast6M'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumInqLast6Mexcl7days\n",
    "Number of Inq Last 6 Months excl 7days. Excluding the last 7 days removes inquiries that are likely due to price comparision shopping.\n",
    "\n",
    "outliers (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use clamping to remove outliers\n",
    "Clamping_IQR('NumInqLast6Mexcl7days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumInqLast6Mexcl7days'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetFractionRevolvingBurden\n",
    "Net Fraction Revolving Burden. This is revolving balance divided by credit limit. A revolving balance is the portion of credit card spending that goes unpaid at the end of a billing cycle.\n",
    "\n",
    "Special Value -8 (2%) & outlier\n",
    "\n",
    "-8 means no Usable/Valid Accounts Trades or Inquiries\n",
    "\n",
    "Since the percentage of special values is quite low, these are replaced with the median value of the feature because, even if the value means no valid inquires, we consider a better approach to assign the median value rather than dropping the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the special value to null so its negative value would not impact the mean value of the feature.\n",
    "df['NetFractionRevolvingBurden'] = df['NetFractionRevolvingBurden'].replace(-8,np.nan)\n",
    "#since invalid data is very low we can use imputation and replace invalid values with the median\n",
    "df['NetFractionRevolvingBurden'] = df['NetFractionRevolvingBurden'].replace(np.nan, np.round(df['NetFractionRevolvingBurden'].median(), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use clamping to remove outliers\n",
    "Clamping_IQR('NetFractionRevolvingBurden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NetFractionRevolvingBurden'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetFractionInstallBurden\n",
    "Net Fraction Installment Burden. This is installment balance divided by original loan amount\n",
    "\n",
    "Special Value -8 (34%)\n",
    "\n",
    "-8 No Usable/Valid Accounts Trades or Inquiries\n",
    "\n",
    "Since for this feature invalid data is very high and replacing the special value using imputation would cause a drastic change in the feature, the decision is to drop the feature. Details about this decision are explained in Data_Quality_Plan.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this feature invalid data is very high and a decision has been made to drop the feature\n",
    "\n",
    "df = df.drop('NetFractionInstallBurden', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumRevolvingTradesWBalance\t\n",
    "Number Revolving Trades with Balance.\n",
    "\n",
    "Revolving trade lines are credit products that creditors can use multiple times. These accounts include credit cards and equity lines. The accounts \"revolve,\" meaning the balances fluctuate from month to month based on usage. The term \"trade\" simply means account. The balance you owe, relative to the maximum line amount, has an impact on your overall credit score. (source: https://www.sapling.com/7839565/do-lines-mean-credit-bureau)\n",
    "\n",
    "Special Value -8 (2%) & Outliers (High)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the special value to null so its negative value would not impact the mean value of the feature.\n",
    "df['NumRevolvingTradesWBalance'] = df['NumRevolvingTradesWBalance'].replace(-8,np.nan)\n",
    "#since invalid data is very low we can use imputation and replace invalid values with the mean\n",
    "df['NumRevolvingTradesWBalance'] = df['NumRevolvingTradesWBalance'].replace(np.nan, np.round(df['NumRevolvingTradesWBalance'].mean(), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use clamping to remove outliers\n",
    "Clamping_IQR('NumRevolvingTradesWBalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumRevolvingTradesWBalance'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumInstallTradesWBalance\t\n",
    "Number Installment Trades with Balance.\n",
    "\n",
    "Installment trade accounts involve agreements you make to pay an account over time.\n",
    "\n",
    "Special Value -8 (9%) & outliers (High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the special value to null so its negative value would not impact the mean value of the feature.\n",
    "df['NumInstallTradesWBalance'] = df['NumInstallTradesWBalance'].replace(-8,np.nan)\n",
    "#since invalid data is quite low we can use imputation and replace invalid values with the mean\n",
    "df['NumInstallTradesWBalance'] = df['NumInstallTradesWBalance'].replace([np.nan], np.round(df['NumInstallTradesWBalance'].mean(), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use clamping to remove outliers\n",
    "Clamping_IQR('NumInstallTradesWBalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumInstallTradesWBalance'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumBank/NatlTradesWHighUtilization\t\n",
    "Number Bank / Natl Trades with high utilization ratio. This counts the number of credit cards on a consumer credit bureau report carrying a balance that is at 75% of its limit or greater.\n",
    "\n",
    "Special Value -8 (7%) & outliers (High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the special value to null so its negative value would not impact the mean value of the feature.\n",
    "df['NumBank2NatlTradesWHighUtilization'] = df['NumBank2NatlTradesWHighUtilization'].replace(-8,np.nan)\n",
    "#since invalid data is low we can use imputation and replace invalid values with the mean\n",
    "df['NumBank2NatlTradesWHighUtilization'] = df['NumBank2NatlTradesWHighUtilization'].replace(np.nan, np.round(df['NumBank2NatlTradesWHighUtilization'].mean(), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use clamping to remove outliers\n",
    "Clamping_IQR('NumBank2NatlTradesWHighUtilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['NumBank2NatlTradesWHighUtilization'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PercentTradesWBalance\n",
    "Percent Trades with Balance\n",
    "\n",
    "Special Values -8 (0.2%) & Outliers (low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the special value to null so its negative value would not impact the mean value of the feature.\n",
    "df['PercentTradesWBalance'] = df['PercentTradesWBalance'].replace(-8,np.nan)\n",
    "#since missing data is low we can use imputation and replace missing values with the mean\n",
    "df['PercentTradesWBalance'] = df['PercentTradesWBalance'].replace(np.nan, np.round(df['PercentTradesWBalance'].mean(), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use clamping to remove outliers\n",
    "Clamping_IQR('PercentTradesWBalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after clamping box plot\n",
    "df['PercentTradesWBalance'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new CSV file with a self explanatory name\n",
    "df.to_csv(\"CreditRisk_clean_round2_5Mar2019_DataQualityPlan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the graphs of the updated and cleaned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of all updated numerical features\n",
    "plt.figure()\n",
    "df.hist(figsize=(20, 20), bins=12)\n",
    "# save the plot into a file as shown below.\n",
    "#plt.savefig('CreditRisk_clean_DataQualityPlan_NumericFeatures_Histograms.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraxt only continuous features\n",
    "numeric_columns = df.select_dtypes(['float64']).columns\n",
    "\n",
    "# plot the box plots of the updated numeric features\n",
    "for elem in numeric_columns:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    df[elem].plot(kind='box')\n",
    "    #plt.savefig(elem+'_clean_DataQualityPlan_NumericFeatures_Boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar plots for categorical features\n",
    "#extraxt only categorical features\n",
    "category_columns = df.select_dtypes(['object'])\n",
    "# Plot a barplot for categorical features\n",
    "for elem in category_columns:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    df[elem].value_counts().plot(kind='bar')\n",
    "    #plt.savefig(elem+'_DataQualityPlan_CategoryFeatures_Bar.png')\n",
    "    plt.suptitle(elem, x=0.5, y=0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Exploring relationships between feature pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading csv file\n",
    "df = pd.read_csv('CreditRisk_clean_round2_5Mar2019_DataQualityPlan.csv')\n",
    "\n",
    "# create dataset only with continuos features\n",
    "df_continuous = df.select_dtypes(['float64'])\n",
    "\n",
    "# create dataset only with cateforical features\n",
    "df_categorical = df.select_dtypes(['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continuous-continuous features\n",
    "\n",
    "since the dataset includes 18 continuous features, in order to extract a subset of features that shows a relationship between these, the analysis included the following steps:\n",
    "- create a complete scatter_matrix with all the features to have a first impression of possible relations\n",
    "- create a continuous correlation heat map with colors that indicate the correlation between each pair \n",
    "- create a table containing the continuous correlation values for every pair, where values close to 1 represent a strong correlation \n",
    "\n",
    "after these three steps a subset of continuos feature that shows a correlation was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter_matrix with all the features to have a first impression of possible relations\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(df_continuous, alpha = 1, figsize=(55,55), diagonal ='hist')\n",
    "#plt.savefig('scatter_matrix_all_continuous_features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn to create a continuous correlation heat map\n",
    "corr = df_continuous.corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table containing the correlation values\n",
    "corr = df_continuous.corr()\n",
    "corr.style.background_gradient(cmap='viridis').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table contains the observed relationship between continuous features and the corresponding correlation values\n",
    "\n",
    "| Continuous Feature     | Continuous Feature   | Correlation Value       |\n",
    "|-------------------------|----------------------|------------------------   |\n",
    "| NumTotalTrades          | NumSatisfactoryTrades | 0.83              |\n",
    "| NumInqLast6Mexcl7days          | NumInqLast6M | 0.99              |\n",
    "| NumTrades90Ever/DerogPubRec          | NumTrades60Ever/DerogPubRec | 0.84              |\n",
    "\n",
    "The study of these relationships indicates a high correlation between <i>**NumTrades90Ever/DerogPubRec**</i> and <i>**NumTrades60Ever/DerogPubRec**</i> as they represent the same feature over a different period of time. \n",
    "\n",
    "The features <i>**NumInqLast6Mexcl7days**</i> and <i>**NumInqLast6M**</i> are strongly correlated with a correlation value very close to 1. Thier strong relationship is due to the fact that they represent the same information with the only difference that <i>**NumInqLast6Mexcl7days**</i> excludes 7 days.\n",
    "\n",
    "A strong correlation is also give by <i>**NumTotalTrades**</i> & <i>**NumSatisfactoryTrades**</i> as they both represent a number of trades. It's however interesting to notice that most of the trades were satisfacory, a \"satisfactory trade\" is one where the borrower has paid on time as agreed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset with the continuous features that show a relation\n",
    "df_cont_cont=df[['NumTotalTrades', 'NumSatisfactoryTrades', 'NumInqLast6Mexcl7days', 'NumInqLast6M', 'NumTrades90Ever2DerogPubRec', 'NumTrades60Ever2DerogPubRec' ]]\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df_cont_cont, alpha = 1, figsize=(20,20), diagonal ='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn to create a continuous correlation heat map\n",
    "corr = df_cont_cont.corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical-categorical features\n",
    "\n",
    "there are five categorical features and RiskPerformance represent the value of the target outcome for each instance, i.e. if the instance is labeled as 'Good' or 'Bad'.\n",
    "In order to better understand the relationships among these features we first use stacked bar plots that compare the target outcome 'Good' or 'Bad' with the values of the other categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from http://queirozf.com/entries/pandas-dataframe-plot-examples-with-matplotlib-pyplot\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.groupby(['RiskPerformance','MaxDelqEver']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar', figsize=(5,5),stacked=True).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('MaxDelqEver', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['RiskPerformance','MaxDelq2PublicRecLast12M']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar', figsize=(5,5),stacked=True).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('MaxDelq/PublicRecLast12M', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['RiskPerformance','MSinceMostRecentDelq']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar', color=['blue','orange', 'green', 'red', 'purple','brown', 'pink', 'grey', 'black', 'gold', 'lightblue'], figsize=(5,5),stacked=True).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('MSinceMostRecentDelq', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['RiskPerformance','MSinceMostRecentInqexcl7days']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar', figsize=(5,5),stacked=True).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('MSinceMostRecentInqexcl7days', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked bar plots show on the X axis the taget outcome 'Good' or 'Bad' and on the Y axis the total percentage of values of the other categorical feature observed. The legend on right hand side of the figure shows, in different colours, the different values for the feature. With this type of graph it's easy to observe that certain values prevale where the target is 'Good' and some other are higher when the target is 'Bad'.\n",
    "\n",
    "In more details, in the first graph that compares <i>**RiskPerformance**</i> & <i>**MaxDelqEver**</i> we observe that:\n",
    "- current and never deliquent (purle) and unknown deliquency (pink) have a higher percentage in the 'Good' instances of the dataset\n",
    "- all the values that indicate a negative status (derogatory comment (brown), 90 days delinquent (red), 60 days delinquent (green), 30 days delinquent (orange) and 120+ days delinquent (blue)), have a higher percentage in the 'Bad' instances of the dataset\n",
    "\n",
    "the second graph that compares <i>**RiskPerformance**</i> & <i>**MaxDelq/PublicRecLast12M**</i> we observe that:\n",
    "- current and never deliquent (brown) has a higher percentage in the 'Good' instances of the dataset\n",
    "- all the other values have a higher percentage in the 'Bad' instances of the dataset\n",
    "\n",
    "the third graph compares <i>**RiskPerformance**</i> & <i>**MSinceMostRecentDelq**</i> and we observe that:\n",
    "- the values no delinquencies (light-blue) appears with a frequency of about 60% in the 'Good' instances while, in the 'Bad' instances of the dataset, it represents only about 40% of the values \n",
    "- the 'Bad' instances have a relevant amount of values, about 40%, that refer to delinquencies that happend in the last 20 months while these values correspond to less than 20% in the 'Good' instances\n",
    "- a common trend that can be recognized is that there is a decrease in frequency towards an increasing number of months since the last delinquency, this indicates that for most of the instances associated with a delinquency, these negative events happend recently.\n",
    "\n",
    "the last graph compares <i>**RiskPerformance**</i> & <i>**MSinceMostRecentInqexcl7days**</i> and we observe that:\n",
    "- for both 'Good' and 'Bad' instances, the highest percentages of values refer to a recent inquiry that happend in the last 4 months. However, this percentage is greater for the 'Bad' intances were it covers about 65% of the data.\n",
    "- it's interesting to observe that, contrary to what expected, the 'Bad' instances have a higher percentage of values labeled as 'No inquiries'\n",
    "- the 'Good' instances show a greater percentage of inquiries that happened between the last 5-9 months, but also show a larger amount of values labeled as no \"hard\" inquiries, i.e. these instances were not actively searching for credit\n",
    "\n",
    "<u>Summary:</u>\n",
    "\n",
    "all these graphs, even if representing different information and displaying different results, indicate a similar trend were the instances labeled as 'Good' are associated with a higher percentage of \"positive\" values, for example 'current and never deliquent'(<i>**MaxDelqEver**</i> & <i>**MaxDelq/PublicRecLast12M**</i>),  'No delinquencies' (<i>**MSinceMostRecentDelq**</i>) or 'No \"hard\" inquiries' (<i>**MSinceMostRecentInqexcl7days**</i>), and a lower frequency of \"negative values\", for example '0-9 months' (<i>**MSinceMostRecentDelq**</i>), '0-4 months'(<i>**MSinceMostRecentInqexcl7days**</i>), '30 days delinquent' and '60 days delinquent' (<i>**MaxDelqEver**</i> & <i>**MaxDelq/PublicRecLast12M**</i>) and others.\n",
    "\n",
    "Therefore we can observe that values that show delinquency status are more likely to be associated with the 'Bad' flag value. On the other hand, values that show no previous or current delinquency are ofter associated with the 'Good' flag value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following graphs we can observe in more details the percentage of each value for the categorical features for the 'Good' and 'Bad' <i>**RiskPerformance**</i> flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract a dataframe with only the categorical features\n",
    "catFeatures = df[['RiskPerformance', 'MaxDelq2PublicRecLast12M', 'MaxDelqEver', 'MSinceMostRecentInqexcl7days', 'MSinceMostRecentDelq']]\n",
    "#extract only data where RiskPerformance flag is 'Good'\n",
    "good = catFeatures.loc[catFeatures['RiskPerformance'] == 'Good']\n",
    "#extract only data where RiskPerformance flag is 'Bad'\n",
    "bad = catFeatures.loc[catFeatures['RiskPerformance'] == 'Bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot that show the percentage of MaxDelqEver values with RiskPerformance = Good\n",
    "good.groupby(['RiskPerformance','MaxDelqEver']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).plot(kind='bar', figsize=(5,5),stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('Good', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot that show the percentage of MaxDelqEver values with RiskPerformance = Bad\n",
    "bad.groupby(['RiskPerformance','MaxDelqEver']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).plot(kind='bar', figsize=(5,5),stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('Bad', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot that show the percentage of MaxDelq/PublicRecLast12M values with RiskPerformance = Good\n",
    "good.groupby(['RiskPerformance','MaxDelq2PublicRecLast12M']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).plot(kind='bar', figsize=(5,5),stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('Good', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot that show the percentage of MaxDelq/PublicRecLast12M values with RiskPerformance = Bad\n",
    "bad.groupby(['RiskPerformance','MaxDelq2PublicRecLast12M']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).plot(kind='bar', figsize=(5,5),stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('Bad', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot that show the percentage of MSinceMostRecentDelq values with RiskPerformance = Good\n",
    "good.groupby(['RiskPerformance','MSinceMostRecentDelq']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).plot(kind='bar', figsize=(5,5),stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('Good', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot that show the percentage of MSinceMostRecentDelq values with RiskPerformance = Bad\n",
    "bad.groupby(['RiskPerformance','MSinceMostRecentDelq']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).plot(kind='bar', figsize=(5,5),stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('Bad', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot that show the percentage of MSinceMostRecentInqexcl7days values with RiskPerformance = Good\n",
    "good.groupby(['RiskPerformance','MSinceMostRecentInqexcl7days']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).plot(kind='bar', figsize=(5,5),stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('Good', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot that show the percentage of MSinceMostRecentInqexcl7days values with RiskPerformance = Bad\n",
    "bad.groupby(['RiskPerformance','MSinceMostRecentInqexcl7days']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).plot(kind='bar', figsize=(5,5),stacked=True)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('Bad', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continuous-categorical features\n",
    "\n",
    "To visualize the relationship between a continuous feature and a categorical feature we use a small multiples approach, drawing a density histogram of the values of the continuous feature for each level of the categorical feature. \n",
    "\n",
    "The continuos feature will be combined with only the categorical feature <i>**RiskPerformance**</i> as it represents the target flag and would be relevant to observe a dependancy with other features.\n",
    "\n",
    "Each histogram includes only those instances in the dataset that have the associated level of the categorical feature, i.e. 'Good' or 'Bad'. If the features are unrelated (or independent) then the histograms for each level should be very similar. If the features are related, however, then the shapes and/or the central tendencies of the histograms will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = df.select_dtypes(['int64', 'float64']).columns\n",
    "continuous_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract a dataframe with only the numerical features and the categorical features RiskPerformance\n",
    "numFeatures = df[['RiskPerformance','ExternalRiskEstimate', 'MSinceOldestTradeOpen',\n",
    "       'MSinceMostRecentTradeOpen', 'AverageMInFile', 'NumSatisfactoryTrades',\n",
    "       'NumTrades60Ever2DerogPubRec', 'NumTrades90Ever2DerogPubRec',\n",
    "       'PercentTradesNeverDelq', 'NumTotalTrades', 'NumTradesOpeninLast12M',\n",
    "       'PercentInstallTrades', 'NumInqLast6M', 'NumInqLast6Mexcl7days',\n",
    "       'NetFractionRevolvingBurden', 'NumRevolvingTradesWBalance',\n",
    "       'NumInstallTradesWBalance', 'NumBank2NatlTradesWHighUtilization',\n",
    "       'PercentTradesWBalance']]\n",
    "#extract only data where RiskPerformance flag is 'Good'\n",
    "good = numFeatures.loc[numFeatures['RiskPerformance'] == 'Good']\n",
    "#extract only data where RiskPerformance flag is 'Bad'\n",
    "bad = numFeatures.loc[numFeatures['RiskPerformance'] == 'Bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histograms for the continuous features considering only the instances flagged as 'Good'\n",
    "good.hist(figsize=(20, 20), bins=12, density=True)\n",
    "plt.suptitle('Instances flagged as \"Good\"', x=0.5, y=0.93, fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histograms for the continuous features considering only the instances flagged as 'Bad'\n",
    "bad.hist(figsize=(20, 20), bins=12, density=True)\n",
    "plt.suptitle('Instances flagged as \"Bad\"', x=0.5, y=0.93, fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison of the graphs shows that, in general, the histograms have a very similar shape either considering only the 'Good' instances or only the 'Bad' ones. \n",
    "Some exceptions are observed in these features:\n",
    "- <i>**ExternalRiskEstimate**</i>: 'Good' instances tend to have a higher value\n",
    "- <i>**NetFractionRevolvingBurden**</i>: 'Good' instances tend to have more values near 0 while 'Bad' instances have a larger range of values above 0\n",
    "- <i>**NumBank/NatlTradesWHighUtilization**</i>: 'Good' instances tend to have more values at 0 or 1, 'Bad' instances have a larger number of values above 0\n",
    "- <i>**PercentTradesNeverDelq**</i>: 'Good' instances have most of the values at 100%, 'Bad' instance show more values below 100%\n",
    "- <i>**PercentTradesWBalance**</i>: 'Bad' instances have a larger number of values near 100%\n",
    "\n",
    "Considering what described above, this analysis indicates that most of the continuous features are unrelated to the target categorical feature <i>**RiskPerformance**</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further analysis can be performed using box plots: for each level of the categorical feature, a box plot of the corresponding values of the continuous feature is drawn. This gives multiple box plots that offer an easy comparison of how the central tendency and variation of the continuous feature change for the different levels of the categorical feature. \n",
    "When a relationship exists between the two features, the box plots should show differing central tendencies and variations. When no relationship exists, the box plots should all appear similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in numFeatures:\n",
    "    if elem != 'RiskPerformance':\n",
    "        plt.figure(figsize=(5,5))\n",
    "        pd.concat([good[elem], bad[elem]], axis=1).boxplot()\n",
    "        plt.title('\"Good\"                                   \"Bad\"', x=0.5, y=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing the box plots, we can confirm the observations made comparing the histograms. \n",
    "\n",
    "We can therefore state that some continuous features show a slight relation/dependency with the values of the target outcome ('Good' or 'Bad'), however, no strong dependency can be observed between any continuos feature and the categorical feature RiskPerformance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 \n",
    "\n",
    "As a first step, the analysis of continous-continous features shows that there is a correlation of 0.99 between <i>**NumInqLast6Mexcl7days**</i> and <i>**NumInqLast6M**</i>. Due to the fact that this two features essentially represent the same information and we want to avoid redundancy of information, the feature <i>**NumInqLast6Mexcl7days**</i> will be dropped.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop feature NumInqLast6Mexcl7days\n",
    "df = df.drop('NumInqLast6Mexcl7days',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, as we have seen in the previous section, the relationships between categorical features indicate a certain behaviour if we consider 'Good' or 'Bad' instances. In order to better capture the target outcome we'll create a new binary feature called <i>**PrevDelq**</i> (Previous Delinquencies) that, based on the values of the features <i>**MaxDelqEver**</i>, <i>**MaxDelq2PublicRecLast12M**</i> and <i>**MSinceMostRecentDelq**</i>, flags with a 'Yes' and instance that had a previous delinquecy indicated by any of the features, and with a 'No' an instance that doesn't show any illegal behaviour in any feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate new column PrevDelq based on comdined conditionals\n",
    "df.loc[(df['MaxDelq2PublicRecLast12M'] != 'current and never delinquent') |\n",
    "       (df['MaxDelqEver'] != 'current and never delinquent') | \n",
    "       (df['MSinceMostRecentDelq'] != 'No delinquencies') , 'PrevDelq'] = 'Yes'\n",
    "\n",
    "df.loc[(df['MaxDelq2PublicRecLast12M'] == 'current and never delinquent') |\n",
    "       (df['MaxDelqEver'] == 'current and never delinquent') | \n",
    "       (df['MSinceMostRecentDelq'] == 'No delinquencies') , 'PrevDelq'] = 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a bar for the new feature created\n",
    "df.groupby(['RiskPerformance','PrevDelq']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar', figsize=(5,5),stacked=True).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('PrevDelq', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from the graph above that more than 60% of the instances flagged as 'Bad' show and history of delinquency while more than 60% of the instances flagged as 'Good' don't show any previous delinquency.\n",
    "\n",
    "We will now create a few new features based on the relationships observed between continuous and categorical features. The features that show the most different behaviour considering 'Good' or 'Bad' instances are: \n",
    "\n",
    "- <i>**ExternalRiskEstimate**</i>\n",
    "- <i>**NetFractionRevolvingBurden: This is revolving balance divided by credit limit. a revolving balance is the portion of credit card spending that goes unpaid at the end of a billing cycle.**</i>\n",
    "- <i>**PercentTradesNeverDelq**</i>\n",
    "\n",
    "We can therefore create three binary features that could better capture the target outcome:\n",
    "\n",
    "<i>**RiskEst**</i>: Risk Estimate, derived from <i>**ExternalRiskEstimate**</i>, where the flag 'High Risk' (<=70) refers to instances with a bad risk value and the flag 'Low Risk' referst to good risk values.\n",
    "\n",
    "<i>**TradesDelq**</i>: Trades Deliquent, derived from <i>**PercentTradesNeverDelq**</i>, where the flag 'No' (100%) refers to instances with no deliquent trades and the flag 'Yes' (<100%) referst to instance that had deliquent trandes.\n",
    "\n",
    "<i>**RevBalance/CreditLimit**</i>: revolving balance divided by credit limit, derived from <i>**NetFractionRevolvingBurden**</i>, where the flag 'High' (>=20) indicates bad behaviour and the flag 'Low' (<20) indicates good behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate new feature RiskEst \n",
    "df.loc[(df['ExternalRiskEstimate'] <= 70), 'RiskEst'] = 'High Risk'\n",
    "df.loc[(df['ExternalRiskEstimate'] > 70), 'RiskEst'] = 'Low Risk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a bar for the new feature created\n",
    "df.groupby(['RiskPerformance','RiskEst']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar', figsize=(5,5),stacked=True).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('RiskEst', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate new feature TradesDelq \n",
    "df.loc[(df['PercentTradesNeverDelq'] < 100), 'TradesDelq'] = 'Yes'\n",
    "df.loc[(df['PercentTradesNeverDelq'] == 100), 'TradesDelq'] = 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a bar for the new feature created\n",
    "df.groupby(['RiskPerformance','TradesDelq']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar', figsize=(5,5),stacked=True).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('TradesDelq', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate new feature RevBalance/CreditLimit \n",
    "df.loc[(df['NetFractionRevolvingBurden'] >= 20), 'RevBalance/CreditLimit'] = 'High'\n",
    "df.loc[(df['NetFractionRevolvingBurden'] < 20), 'RevBalance/CreditLimit'] = 'Low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a bar for the new feature created\n",
    "df.groupby(['RiskPerformance','RevBalance/CreditLimit']).size().groupby(level=0).apply(\n",
    "    lambda x: 100 * x / x.sum()).unstack().plot(kind='bar', figsize=(5,5),stacked=True).legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.suptitle('RevBalance/CreditLimit', x=0.5, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from the graphs above that these three new features can help in identify the target outcome. The 'negative' flags, set in each of the binary feature, always have a considerably higher frequency in the 'Bad' instances while the 'positive' flags are always dominant in the 'Good' instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display target outcome and new features\n",
    "df[['RiskPerformance','RiskEst','PrevDelq','TradesDelq','RevBalance/CreditLimit']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save updated dataframe to csv file\n",
    "#df.to_csv(\"CreditRisk_final_07Mar2019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "Fundamentals of Machine Learning for Predictive Data Analytics, by J. Kelleher, B. Mac Namee and A. D’Arcy, MIT Press, 2015 (machinelearningbook.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
